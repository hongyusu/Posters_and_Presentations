



\documentclass[first=dgreen,second=purple,logo=yellowexc]{aaltoslides}
%\documentclass{aaltoslides} % DEFAULT
%\documentclass[first=purple,second=lgreen,logo=redque,normaltitle,nofoot]{aaltoslides} % SOME OPTION EXAMPLES



\input{slide_macro_su.tex}




\title{Structured Output Learning with A Random Sample of Spanning Trees}
\author{Hongyu Su}



\institute[ICS]{
Helsinki Institute for Information Technology HIIT\\
Department of Information and Computer Science\\
Aalto University
}

\aaltofootertext{Random Spanning Tree Approximation}{\today}{\arabic{page}/\pageref{LastPage}\ }


\date{ \today} %\date{Version 1.0, \today}

\iffalse
\AtBeginSection[]
{
  \begin{frame}<beamer>{Outline}
    \tableofcontents[currentsection,subsection]
  \end{frame}
}
\fi




%--------------------------------
%
% document
%
%--------------------------------

\begin{document}

\aaltotitleframe



%
\begin{frame}{Multilabel Classification}
	\begin{itemize}
		\item Multilabel classification is an important research field in machine learning.
		\begin{itemize}
			\item For example, a document can be classified as ``science'', ``genomics'', and ``drug discovery''.
			\item Each input variable $\vx\in\vXcal$ is simultaneously associated with multiple output variables $\vy\in\vYcal,\vYcal=\Ycal_1\times\cdots\times\Ycal_l$.
			\item The goal is to find a mapping function that predicts the best values of an output given an input $f\in\Hcal:\vXcal\rightarrow\vYcal$.
		\end{itemize}
		\item The central problems of multilabel classification:
		\begin{itemize}
			\item The size of the output space $\vYcal$ is exponential in the number of microlabels.
			\item The dependency of microlabels needs to be exploited to improve the prediction performance.
		\end{itemize}
	\end{itemize}
\end{frame}



%
\begin{frame}{Flat Multilabel Classification}
	\begin{itemize}
		\item Multiple output variables are treated as a ``flat'' vector.
		\item For example, \mlknn, \adaboostmh, \mtl, ...
		\item It is difficult to take into consideration the correlation of labels.
	\end{itemize}
\end{frame}

\begin{frame}{Structured Output Learning}
	\begin{itemize}
		\item There is an \textit{output graph} connecting multiple labels.
		\begin{itemize}
			\item A set of nodes corresponds to the multiple labels.
			\item A set of edges represents the correlation between labels.
		\end{itemize}
		\item Hierarchical classification:
		\begin{itemize}
			\item The output graph is a rooted tree or a directed graph defining the different levels of granularities.
			\item For example, \svmstruct, ...
		\end{itemize}
		\item Graph labeling:
		\begin{itemize}
			\item The output graph often takes a general form (e.g., a tree, a chain).
			\item For example, \mmmn, \crf, \mmcrf, ...
		\end{itemize}
		\item The output graph is assumed to be known \textit{apriori}.
	\end{itemize}
\end{frame}



%
\begin{frame}{Research Question}
	\begin{itemize}
		\item The output graph is hidden in many applications.
		\begin{itemize}
			\item For example, a surveillance photo can be tagged with ``building'', ``road'', ``pedestrian'', and ``vehicle''.
		\end{itemize}
		\item We focus on the problem in structured output learning when the output graph is not observed.
		\item Our approach:
		\begin{itemize}
			\item Assume the dependency can be modeled by a complete set of pairwise correlations.
			\item Build a structured output learning model with a complete graph as the output graph.
			\item Solve the optimization problem and the inference problem.
		\end{itemize}
	\end{itemize}
\end{frame}



%
\begin{frame}{Contributions}
	\begin{itemize}
		\item A structured output learning model which performs max-margin learning on a random sample of spanning tree.
		\item The model is not constrained to the availability of the output graph.
		\item The \nphard\ inference problem can be solved by a polynomial time algorithm with a condition guaranteeing the exact solution.
		\item The theoretical analysis and the empirical results verify the performance of the proposed model.
	\end{itemize}
\end{frame}



%
\begin{frame}[allowframebreaks]{Model}
	\begin{itemize}
		\item The training examples are given in pair $S=\{(\vx_i,\vy_i)\}_{i=1}^{m}$.
		\item A complete graph $G=(E,V)$ is used as the output graph.
		\item $\Gamma_G(\vy_i)$ is the output feature map on a complete graph $G$.
		\item Each example is mapped to a joint feature space by a joint feature map
		\begin{align*}
			\phi_G(\vx_i,\vy_i) = \varphi(\vx_i)\otimes\Gamma_G(\vy_i).
		\end{align*}
		\item A compatibility score is defined as
		\begin{align*}
			F(\vx,\vy;\vw) = \ip{\vw}{\phi_G(\vx,\vy)}=\sum_{e\in E}\ip{\vw_e}{\phi_G(\vx,\vy_{e})}
		\end{align*} 
		\item $\vw$ ensures an input $\vx_i$ with a correct multilabel $\vy_i$ achieves a higher score than with any incorrect multilabel $\vy\in\Ycal$.
		\item The predicted output $\vy(\vx)$ for a given input $\vx$ is computed by
		\begin{align*}
			\vy(\vx) = \underset{\vy\in\vYcal}{\argmax}\,F(\vx,\vy;\vw) = \underset{\vy\in\vYcal}{\argmax}\,\ip{\vw}{\phi_G(\vx,\vy)},
		\end{align*}
		which is called \textit{inference problem}.
		\item The {inference problem} is \nphard\ for most joint feature maps on the complete graph.
	\end{itemize}
\end{frame}



%
\begin{frame}{Learning with A Complete Graph}
	\begin{itemize}
		\item The \textit{margin} of an example $\vx_i$ is
		\begin{align*}
			\gamma_G(\vx_i;\vw) = \underset{\vy\in\vYcal}{\minimize}\,[F(\vx_i,\vy_i;\vw)-F(\vx_i,\vy;\vw)].
		\end{align*}
		\item $\vw$ is solved by \textit{maximum-margin principle} which aims to maximize $\gamma(\vx_i;\vw)$ over all training example.
		\item The problem includes:
		\begin{itemize}
			\item The \nphardness\ of the inference problem on a complete graph.
			\item A large parameter space: $\Theta(k^2)$
		\end{itemize}
		\item We aim to use a joint feature map that allows the inference problem be solved in polynomial time.
	\end{itemize}
\end{frame}



%
\begin{frame}{Random Spanning Tree Approximation}
	\begin{itemize}
		\item We proved if a large margin structured output predictor exists, then combining a small sample of random trees will, with a high probability, generate a predictor with good generalization.
		\item $\Tcal=\{T_1,\cdots,T_n\}$ is a set of spanning trees randomly sampled from the complete graph $G$.
		\item The compatibility score can be re-defined based on $\Tcal$ as
		\begin{align*}
			F_{\Tcal}(\vx_i,\vy_i;\vw_{T_t}) = \sum_{t=1}^{n}\ip{\vw_{T_t}}{\phi_{T_t}(\vx_i,\vy_i)}.
		\end{align*}
		\item The inference problem of predicting the output $\vy_{\Tcal}(\vx)$ for a given input $\vw$ is 
		\begin{align*}
			\vy_{\Tcal}(\vx) = \underset{\vy\in\vYcal}{\argmax}\,F_{\Tcal}(\vx,\vy;\vw_{\Tcal})=\underset{\vy\in\vYcal}{\argmax}\,\sum_{t=1}^{n}\ip{\vw_{T_t}}{\phi_{T_t}(\vx,\vy)}.
		\end{align*}
	\end{itemize}
\end{frame}


%
\begin{frame}{Optimization Problem}
	\begin{itemize}
		\item The margin of an example $\vx_i$ achieved by $\Tcal$ is
		\begin{align*}
		\gamma_{\Tcal}(\vx_i;\vw_{\Tcal}) = \underset{\vy\in\vYcal}{\minimize}\,[\sum_{t=1}^{n}\ip{\vw_{T_t}}{\phi_{T_t}(\vx_i,\vy_i)}-\sum_{t=1}^{n}\ip{\vw_{T_t}}{\phi_{T_t}(\vx_i,\vy)}].
	\end{align*}
		\item To learn $\vw_{T_t},\forall T_t\in\Tcal$ we solve the optimization problem
		\begin{align*}
			\underset{\vw_{T_t},\xi_i}{\minimize} & \quad \frac{1}{2}\sum_{t=1}^{n}\norm{\vw_{T_t}}^2 + C\sum_{i=1}^{m}\xi_i\\
			\st & \quad \sum_{t=1}^{n}{ \langle \vw_{T_t}, \phib_{T_t}(\vx_i,\vy_i) \rangle} - \underset{\vy \neq \vy_i}{\maximize\ } \sum_{t=1}^{n}{\langle \vw_{T_t}, \phib_{T_t}(\vx_i,\vy) \rangle } \geq 1 -  \xi_i, \\
			& \quad \xi_i\ge0\, , \forall\ i \in \set{1,\dots,m},
		\end{align*}
	\end{itemize}
\end{frame}



%
\begin{frame}{Inference Problem}
	\begin{itemize}
		\item The inference problem of \rta\ is defined as finding the multilabel $\vy_{\Tcal}(\vx)$ that maximizes the sum of scores over a collection of trees
		\begin{align*}
			\vy_{\Tcal}(\vx) = \underset{\vy\in\vYcal}{\argmax}\,{\color{aaltoblue}F_{\Tcal}(\vx,\vy;\vw_{\Tcal})} = \underset{\vy\in\vYcal}{\argmax}\,\sum_{t=1}^{n}\ip{\vw_{T_t}}{\phi_{T_t}(\vx,\vy)}.
		\end{align*}
		\item The inference problem on each individual spanning tree can be solve efficiently in $\Theta(l)$ by \textit{dynamic programming}
		\begin{align*}
			\vy_{T_t}(\vx) = \underset{\vy\in\vYcal}{\argmax}\,{\color{aaltored}F_{T_t}(\vx,\vy;\vw_{T_t})}= \underset{\vy\in\vYcal}{\argmax}\,\ip{\vw_{T_t}}{\phi_{T_t}(\vx,\vy)}.
		\end{align*}
		\item There is no guarantee that there exists a tree $T_t\in\Tcal$ for which the maximizer of ${\color{aaltored}F_{T_t}}$ is the maximizer of ${\color{aaltoblue}F_{\Tcal}}$.
	\end{itemize}
\end{frame}




%
\begin{frame}[allowframebreaks]{Fast Inference Over a Collection of Trees}
	\begin{itemize}
		\item For each tree $T_t$, instead of computing the best multilabel $\vy_{T_t}$, we compute $K$-best multilabels in $\Theta(Kl)$ time
		\begin{align*}
			\Ycal_{T_t,K} = \{\vy_{T_t,1},\cdots,\vy_{T_t,K}\}.
		\end{align*}
		\item Performing the same computation on all trees gives a candidate list of $n\times K$ multilabels in $\Theta(nKl)$ time
		\begin{align*}
			\Ycal_{\Tcal,K}=\Ycal_{T_1,K}\cup\cdots\Ycal_{T_n,K}.
		\end{align*}
		\item For now, we assume the best scoring multilabel of a collection of trees exists in the list $\Ycal_{\Tcal,K}$. 
		\item Assume 
		\begin{align*}
		\vy_K^*=\underset{\vy\in\Ycal_{\Tcal,K}}{\argmax}\,F_{\Tcal}(\vx,\vy;\vw_{\Tcal}).
		\end{align*} 
		If
		\begin{align*}
			F_{\Tcal}(\vx,\vy_K^*;\vw_{\Tcal})\ge\frac{1}{n}\sum_{t=1}^{n}F_{T_t}(\vx,\vy_{T_t,K},\vw_{T_t})=\theta_{\vx}(K),
		\end{align*}
		then 
		\begin{align*}
			F_{\Tcal}(\vx,\vy_K^*;\vw_{\Tcal}) = \underset{\vy\in\vYcal}{\maximize}\,F_{\Tcal}(\vx,\vy;\vw_{\Tcal}).
		\end{align*}
		\framebreak
		\item For example, $\Tcal=\{T_1,T_2\},\vYcal=\Ycal_1\times\Ycal_2,\Ycal_i=\{+,-\}$
		\begin{table}
			\begin{tabular}{|c|cc|cc|c|} \hline
				\multirow{2}{*}{} & \multicolumn{2}{c|}{$T_1$} & \multicolumn{2}{c|}{$T_2$} & $\theta_{\vx}(K)$\\ \cline{2-5}
				& $\vy_{T_1,K}$ & $F_{T_1}(\vx,\vy_{T_1,K})$ & $\vy_{T_1,K}$ & $F_{T_1}(\vx,\vy_{T_1,K})$ & \\ \hline
				$K=1$ & $+-$ & $5$ & $--$ & $4$ & $9$\\ \hline
				$K=2$ & $++$ & $4$ & $-+$ & $3$ & $7$\\ \hline
				$K=3$ & $-+$ & $3$ & $++$ & $3$ & $6$\\ \hline
				$K=4$ & $--$ & $3$ & $+-$ & $2$ & $5$\\ \hline
			\end{tabular}
		\end{table}
		\item We proved that with a high probability $\vy_{\Tcal}$ will appear in $\Ycal_{\Tcal,K}$.
	\end{itemize}
\end{frame}



%
\begin{frame}{Performance of the Inference Algorithm}
	\begin{itemize}
		\item $10$ datasets, $|\Tcal| = \{5,10,40\}, K=\{2,4,8,16,32,40,60\}$
		\item X-axis is the percentage of examples with exact inference.
		\item Y-axis is the value of $K$ as the percentage of the number of microlabels.
		\item $K=100\%|Y|$ corresponds to a complexity of $\Theta(nl^2)$.
	\end{itemize}
	\begin{figure}
		\begin{center}
			\includegraphics[width=11cm]{./result_plot.pdf}
			%\caption{Percentage of examples with provably optimal $\vy$ being in the $K$-best lists plotted as a function of K, scaled with respect to the number of microlabels in the dataset.}
		\end{center}
	\end{figure}
\end{frame}



%
\begin{frame}{Prediction Performance}
	\begin{figure}
		\begin{center}
			\includegraphics[width=11cm]{./result_table.pdf}
			\caption{Prediction performance of each algorithm in terms of microlabel loss and 0/1 loss. The best performing algorithm is highlighted with boldface, the second best is in italic}
		\end{center}
	\end{figure}
\end{frame}



%
\begin{frame}{Conclusions}
	\begin{itemize}
		\item Theoretical study shows if a large margin structured output learner exists,  then the combination of a random sample of spanning trees will achieve a similar margin with a high probability.
		\item The $K$-best inference algorithm is tractable which is proved theoretically and empirically.
		\item \rta\ is not constrained by the availability of the output graph, it can therefore be applied to a wider range of multilabel classification problem where the output graph is believed to play an important role during learning.
	\end{itemize}
\end{frame}




\iffalse
\begin{frame}[allowframebreaks]{Bibliography}
	%\bibliographystyle{plain}
	\bibliographystyle{apalike}
	\bibliography{example}
\end{frame}
\fi


\end{document}
