



\documentclass[first=dgreen,second=purple,logo=yellowexc]{aaltoslides}
%\documentclass{aaltoslides} % DEFAULT
%\documentclass[first=purple,second=lgreen,logo=redque,normaltitle,nofoot]{aaltoslides} % SOME OPTION EXAMPLES



\input{slide_macro_su.tex}




\title{Newton update in L$_2$-norm random tree approximation}
\author{Hongyu Su}



\institute[ICS]{
Helsinki Institute for Information Technology HIIT\\
Department of Computer Science\\
Aalto University
}

%\aaltofootertext{Random Spanning Tree Approximation}{\today}{\arabic{page}/\pageref{LastPage}\ }
\aaltofootertext{Newton update in \rta}{\today}{\arabic{page}}


\date{ \today} %\date{Version 1.0, \today}

\iffalse
\AtBeginSection[]
{
  \begin{frame}<beamer>{Outline}
    \tableofcontents[currentsection,subsection]
  \end{frame}
}
\fi




%--------------------------------
%
% document
%
%--------------------------------

\begin{document}


\aaltotitleframe
\footnotesize


\begin{frame}{Preliminaries}
	\begin{itemize}\footnotesize
		\item $\Xcal$ is an arbitrary input space, $\vx\in\vXcal$.
		\item $\Ycal$ is an output space of a set of $\ell$-dimensional {\em multilabels}
		\begin{align*}\footnotesize
			\vy=(y_1,\cdots,y_{\ell})\in\vYcal.
		\end{align*}
		\item $y_i$ is a {\em microlabel} and $y_i\in\{1,\cdots,r_i\}, r_i\in\ZZ$.
		\item For example, multilabel binary classification $y_i\in\{-1,+1\}$.
		\item Training examples are sampled from $(\vx,\vy)\in\vXcal\times\vYcal$.
		\item Each example $(\vx,\vy)$ is mapped into a joint feature space $\phib(\vx,\vy)$.
		\item $\vw$ is the weight vector in the joint feature space.
		\item Define a linear score function $F(\vw,\vx,\vy) = \ip{\vw}{\phib(\vx,\vy)}$.
		\item The prediction $\vy_{\vw}(\vx)$ of an input $\vx$ is the multilabel $\vy$ that maximizes the score function 
		\begin{align}\footnotesize
			\vy_{\vw}(\vx) = \underset{\vy\in\vYcal}{\argmax}\,\ip{\vw}{\phib(\vx,\vy)}. \label{inference}
		\end{align}
		\item (\ref{inference}) is called {\em inference} problem which is \nphard\ for most output feature maps.
	\end{itemize}
\end{frame}

\begin{frame}{Markov network}
	\begin{itemize}\footnotesize
		\item We assume that the output feature map $\phib$ is a potential function on a Markov network $G=(E,V)$.
		\item $G$ is a complete graph with $|V| = \ell$ nodes and $|E| = \frac{\ell(\ell-1)}{2}$ undirected edges.
		% \item Joint feature map is $\phib(\vx,\vy) = \varphib(\vx)\otimes\psib(\vy)$.
		\item $\varphib(\vx)$ is the input feature map, e.g., bag-of-words feature of an example $\vx$.
		\item $\psib(\vy)$ is the output feature map which is a collection of edges and labels
		\begin{align*}\footnotesize
			\varphib(\vy) = (u_{e})_{e\in E},u_e\in\{-1,+1\}^2.
		\end{align*}
		\item The joint feature is the Kronecker product of $\varphib(\vx)$ and $\psib(\vy)$
		\begin{align*}\footnotesize
			\phib(\vx,\vy) = (\phib_e(\vx,\vy))_{e\in E}=(\varphib(\vx)\otimes\psib_e(\vy_e))_{e\in E}.
		\end{align*}
		\item The score function is 
		\begin{align*}
			F(\vw,\vx,\vy) = \ip{\vw}{\phib(\vx,\vy)} = \sum_{e\in E}\ip{\vw_e}{\phib_e(\vx,\vy_e)}.
		\end{align*}
	\end{itemize}
\end{frame}

\begin{frame}{Inference in terms of all spanning trees}
	\begin{itemize}
		\item Solving the following inference problem on a complete graph is \nphard
		\begin{align*}
			\vy_{\vw}(\vx) = \underset{\vy\in\vYcal}{\argmax}\,F(\vw,\vx,\vy)  = \underset{\vy\in\vYcal}{\argmax}\,\sum_{e\in E}\ip{\vw_e}{\phib_e(\vx,\vy_e)}. 
		\end{align*}
		\item For a complete graph, there are $\ell^{\ell-2}$ unique spanning trees.
		\item We can write $F(\vw,\vx,\vy)$ as a conic combination of all spanning trees
		\begin{align*}
			F(\vw,\vx,\vy) &= \underset{T\in U(G)}{\E}a_T\ip{\vw_T}{\phib_T(\vx,\vy)}\\
			  &\underset{T\in U(G)}{\E}a_T^2=1,  \underset{T\in U(G)}{\E}a_T<1.
		\end{align*}
		\item $U(G)$ is the uniform distribution over $\ell^{\ell-2}$ spanning trees.
		\item There is a exponential dependency on the number of spanning trees.
	\end{itemize}
\end{frame}

\begin{frame}{A sample of $n$ spanning trees}
	\begin{itemize}\footnotesize
		\item Instead of using all spanning trees, we can just use $n$ spanning trees
		\begin{align*}\footnotesize
			F_{\Tcal}(\vw,\vx,\vy) &= \frac{1}{n}\sum_{i=1}^{n}a_{T_i}\ip{\vw_{T_i}}{\phib_{T_i}(\vx,\vy)}\\
			  &\frac{1}{n}\sum_{i=1}^{n}a_{T_i}^2=1,  \frac{1}{n}\sum_{i=1}^{n}a_{T_i}<1.
		\end{align*}
		\item When
		\begin{align*}\footnotesize
			n\ge\frac{\ell^2}{\epsilon^2}(\frac{1}{16}+\frac{1}{2}\ln\frac{8\sqrt{n}}{\delta}),
		\end{align*}
		with high probability, we have $|F_{\Tcal}(\vw,\vx,\vy)-F(\vw,\vx,\vy)|\le\epsilon$.
		\item A sample of $n\in\Theta(\ell^2/\delta^2)$ random spanning tree is sufficient to estimate the score function.
		\item Margin achieved by $F(\vw,\vx,\vy)$ is also preserved by the sample of $n$ random spanning trees $F_{\Tcal}(\vw,\vx,\vy)$.
	\end{itemize}
\end{frame}


\begin{frame}{Optimization problem}
	\begin{itemize}\footnotesize
		\item The primal optimization problem is defined as
		\begin{align*}\footnotesize
			\underset{\vw_{T_i},\xi_i}{\minimize} & \quad \frac{1}{2}\sum_{i=1}^{n}\norm{\vw_{T_i}}^2 + C\sum_{k=1}^{m}\xi_k\\
			\st & \quad \frac{1}{\sqrt{n}}\sum_{i=1}^{n}{ \langle \vw_{T_i}, \phib_{T_t}(\vx_k,\vy_k) \rangle} - \underset{\vy \neq \vy_k}{\maximize\ } \frac{1}{\sqrt{n}}\sum_{i=1}^{n}{\langle \vw_{T_t}, \phib_{T_i}(\vx_k,\vy) \rangle } \geq 1 -  \xi_k, \\
			& \quad \xi_k\ge0\, , \forall\ k \in \set{1,\dots,m}.
		\end{align*}
		\item The marginalized dual problem is defined as
		\begin{align*}\footnotesize
			\underset{\vmu\in\Mcal}{\maximize} & \quad \sum_{i=1}^{n}\left( \vmu_{T_i}\vell_{T_i} - \frac{1}{2}\vmu_{T_i}K_{T_i}\vmu_{T_i}\right)\\
			\st & \quad \sum_{u_e}\vmu_{T_i,e}(u_e)\le C.
		\end{align*}
	\end{itemize}
\end{frame}

\begin{frame}{Optimization algorithm for a single spanning tree}
	\begin{itemize}\footnotesize
		\item We can solve the optimization problem efficiently for each individual spanning tree. 
		\item The algorithm iterates over all training example until convergence.
		\item For the $k$th iteration:
		\begin{enumerate}\footnotesize
			\item Obtain the solution of the $j$th example in $k$th iteration $\vmu_{T_i}^k(j)$.
			\item Compute the gradient $g_{T_i}^k(j) = \ell_{T_i}(j) - K_{T_i}\vmu_{T_i}^k(j)$.
			\item Compute the update direction 
			\begin{align*}
				\hat{\vmu}_{T_i}^{k+1}(j) = \underset{\vmu\in\Mcal}{\argmax}\,\vmu\tp g_{T_i}^k(j).
			\end{align*}
			\item Compute the difference $\Delta\vmu_{T_i}^{k+1}(j) = \hat{\vmu}_{T_i}^{k+1}(j) - \hat{\vmu}_{T_i}^{k}(j)$.
			\item Perform the update $\vmu_{T_i}^{k+1}(j) = \vmu_{T_i}^k(j) + \tau\Delta\vmu_{T_i}^{k+1}(j)$
		\end{enumerate}
		\item The step size along the update direction $\tau$ is given by the exact line search.
		\begin{align*}
			\frac{\partial \left(f(\vmu_{T_i}^{k+1}(j))-f(\vmu_{T_i}^k(j))\right)}{\partial \tau} = 0, 0\le\tau\le1.
		\end{align*}
	\end{itemize}
\end{frame}


\begin{frame}{$\kappa$-best inference for a collection of $n$ spanning trees}
	\begin{itemize}\footnotesize
		\item The algorithm iterates over all training example until convergence.
		\item For the $k$th iteration:
		\begin{enumerate}\footnotesize
			\item Obtain the solutions of the $j$th example over all trees $(\vmu_{T_i}^k(j)){\color{aaltored}_{i=1}^n}$.
			\item Compute the gradients over all trees $(g_{T_i}^k(j)){\color{aaltored}_{i=1}^n}$.
			\item Compute the update directions
			\begin{align*}
				\hat{\vmu}_{T_i}^{k+1}(j) = \underset{\vmu\in\Mcal}{\argmax}\,\vmu\tp g_{T_i}^k(j),\,{\color{aaltored}\forall i}.
			\end{align*}
			\item Compute the best direction
			\begin{align*}
				\tilde{\vmu}_{T_i}^{k+1}(j) = \underset{\vmu\in(\hat{\vmu}_{T_i}^{k+1}(j)){\color{aaltored}_{i=1}^n}}{\argmax}\,{\color{aaltored}\sum_{i=1}^n}\vmu\tp g_{T_i}^k(j)
			\end{align*}
			\item Compute the difference $\Delta\vmu_{T_i}^{k+1}(j) = \hat{\vmu}_{T_i}^{k+1}(j) - \hat{\vmu}_{T_i}^{k}(j),\,{\color{aaltored}\forall i}$.
			\item Perform the update $\vmu_{T_i}^{k+1}(j) = \vmu_{T_i}^k(j) + \tau\Delta\vmu_{T_i}^{k+1}(j),\,{\color{aaltored}\forall i}.$
		\end{enumerate}
		\item The step size along the update direction $\tau$ is given by the exact line search.
		\begin{align*}\footnotesize
			\frac{\partial \left({\color{aaltored}\sum_{i=1}^n}f(\vmu_{T_i}^{k+1}(j))-{\color{aaltored}\sum_{i=1}^n}f(\vmu_{T_i}^k(j))\right)}{\partial \tau} = 0, 0\le\tau\le1.
		\end{align*}
	\end{itemize}
\end{frame}


\begin{frame}{Update with multiple directions}
	\begin{itemize}\footnotesize
		\item The algorithm iterates over all training example until convergence.
		\item For the $k$th iteration:
		\begin{enumerate}\footnotesize
			\item Obtain the solutions of the $j$th example over all trees $(\vmu_{T_i}^k(j)){\color{aaltored}_{i=1}^n}$.
			\item Compute the gradients over all trees $(g_{T_i}^k(j)){\color{aaltored}_{i=1}^n}$.
			\item Compute the update directions
			\begin{align*}
				\hat{\vmu}_{T_i}^{k+1}(j) = \underset{\vmu\in\Mcal}{\argmax}\,\vmu\tp g_{T_i}^k(j),\,{\color{aaltored}\forall i}.
			\end{align*}
			\item Define a conic combination of update directions 
			\begin{align*}
				\tilde{\vmu}_{\color{aaltoblue}G}^{k+1}(j) {\color{aaltoblue}\leftarrow} \sum_{i=1}^{n}\tau_i\hat{\vmu}_{T_i}^{k+1}(j)
			\end{align*}
			\item Compute the difference $\Delta\vmu_{T_i}^{k+1}(j) = \hat{\vmu}_{T_i}^{k+1}(j) - \hat{\vmu}_{T_i}^{k}(j),\,{\color{aaltored}\forall i}$.
			\item Perform the update $\vmu_{T_i}^{k+1}(j) = \vmu_{T_i}^k(j) + \tau\Delta\vmu_{T_i}^{k+1}(j),\,{\color{aaltored}\forall i}.$
		\end{enumerate}
		\item The step size along the update direction $\tau$ is given by the exact line search.
		\begin{align*}\footnotesize
			\frac{\partial \left({\color{aaltored}\sum_{i=1}^n}f(\vmu_{T_i}^{k+1}(j))-{\color{aaltored}\sum_{i=1}^n}f(\vmu_{T_i}^k(j))\right)}{\partial \tau} = 0, 0\le\tau\le1.
		\end{align*}
	\end{itemize}
\end{frame}


% \begin{frame}{}
% 	\begin{itemize}
% 	\end{itemize}
% \end{frame}


% \begin{frame}{}
% 	\begin{itemize}
% 	\end{itemize}
% \end{frame}


% \begin{frame}{}
% 	\begin{itemize}
% 	\end{itemize}
% \end{frame}


% \begin{frame}{}
% 	\begin{itemize}
% 	\end{itemize}
% \end{frame}


% \begin{frame}{}
% 	\begin{itemize}
% 	\end{itemize}
% \end{frame}


% \begin{frame}{}
% 	\begin{itemize}
% 	\end{itemize}
% \end{frame}


%
\begin{frame}{Conclusions}
	\begin{itemize}\footnotesize
		\item 
	\end{itemize}
\end{frame}




\iffalse
\begin{frame}[allowframebreaks]{Bibliography}
	%\bibliographystyle{plain}
	\bibliographystyle{apalike}
	\bibliography{example}
\end{frame}
\fi


\end{document}
