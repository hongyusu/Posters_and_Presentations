



\documentclass[first=dgreen,second=purple,logo=yellowexc]{aaltoslides}
%\documentclass{aaltoslides} % DEFAULT
%\documentclass[first=purple,second=lgreen,logo=redque,normaltitle,nofoot]{aaltoslides} % SOME OPTION EXAMPLES



\input{slide_macro_su.tex}




\title{Structured output prediction for multilabel classification}
\author{Hongyu Su}



\institute[ICS]{
Helsinki Institute for Information Technology HIIT\\
Department of Computer Science, Aalto University
}

\aaltofootertext{Structured output prediction}{\today}{\arabic{page}\ }


\date{ \today} %\date{Version 1.0, \today}

\iffalse
\AtBeginSection[]
{
  \begin{frame}<beamer>{Outline}
    \tableofcontents[currentsection,subsection]
  \end{frame}
}
\fi




%--------------------------------
%
% document
%
%--------------------------------

\begin{document}

\aaltotitleframe
\footnotesize

%
\begin{frame}{Multilabel classification}
	\begin{itemize}\footnotesize
		\item {\em Multilabel classification} is an important research field in machine learning.
		\item Input variable $\vx\in\vXcal$ is in $d$ dimensional input space $\vXcal=\RR^d$.
		\item Output variable $\vy=(y_1,\cdots,y_l)\in\vYcal$ is a binary vector consist of $l$ binary variables $y_j\in\{+1,-1\}$.
		\item $\vy$ is called a multilabel, $y_j$ is called a microlabel.
		\item Output space is composed by a Cartesian product of $l$ sets
		\begin{align*}
			\vYcal=\Ycal_1\times\cdots\times\Ycal_l,\,\Ycal_i=\{+1,-1\}.
		\end{align*}
		\item For example, in document classification, a document $\vx$ can be classified as ``news'', ``movie'', and ``science''
		\begin{align*}
\vy=(\underbrace{+1}_{\text{news}},\underbrace{+1}_{\text{movie}},\underbrace{-1}_{\text{sports}},\underbrace{-1}_{\text{politics}},\underbrace{-1}_{\text{finance}},\underbrace{+1}_{\text{science}},\underbrace{-1}_{\text{art}}).
		\end{align*}\footnotesize
		\item The goal is to find a mapping function $f\in\Hcal$ that predicts the best values of an output given an input $f:\vXcal\rightarrow\vYcal$.
	\end{itemize}
\end{frame}

%
\begin{frame}{Central problems in multilabel classification}
	\begin{itemize}\footnotesize
		\item The size of the output space (searching space) is exponential in the number of microlabels.
		\begin{align*}
			\vYcal=\Ycal_1\times\cdots\times\Ycal_l,\,\Ycal_i=\{+1,-1\}\quad|\vYcal| = 2^l.
		\end{align*}
		\item The dependency of microlabels needs to be exploited to improve the prediction performance.
		\begin{itemize}\footnotesize
			\item If a document is about ``movie'', then it is more likely to be about ``art'' than ``science''.
		\end{itemize}
	\end{itemize}
\end{frame}

%
\begin{frame}{Real world applications}
	\begin{itemize}\footnotesize
		\item Social network, information can spread through multiple users. 
		\begin{tabular}{p{3cm}p{10cm}} 
	    \multirow{2}{*}{\includegraphics[scale = 0.06]{./figures/facebookvideo.png}} & \\
		& $\vy=(\underbrace{+1}_{\text{Ted}},\underbrace{-1}_{\text{Alice}},\underbrace{+1}_{\text{David}},\underbrace{-1}_{\text{Mark}},\underbrace{+1}_{\text{Alex}},\underbrace{-1}_{\text{Zoe}},\underbrace{-1}_{\text{Frank}})$\\
	    \end{tabular}
		\item Image annotation, an image can associate with multiple tags.
		\begin{tabular}{p{3cm}p{10cm}}
        \multirow{2}{*}{\includegraphics[scale = 0.11]{./figures/boatsea.png}} & \\
		& $\vy=(\underbrace{+1}_{\text{boat}},\underbrace{+1}_{\text{sea}},\underbrace{-1}_{\text{sun}},\underbrace{-1}_{\text{beach}},\underbrace{-1}_{\text{people}},\underbrace{+1}_{\text{ice}},\underbrace{+1}_{\text{land}})$\\
        \end{tabular}
		\item Document classification, an article can be assigned to multiple categories.
		\begin{tabular}{p{3cm}p{10cm}} 
        \multirow{2}{*}{\includegraphics[scale = 0.11]{./figures/titanic.jpg}} & \\
		& $\vy=(\underbrace{+1}_{\text{news}},\underbrace{+1}_{\text{economics}},\underbrace{-1}_{\text{sports}},\underbrace{-1}_{\text{politics}},\underbrace{-1}_{\text{movie}},\underbrace{-1}_{\text{science}},\underbrace{-1}_{\text{art}})$\\
        \end{tabular}
		\item Drug discovery, a drug can be effective for multiple symptoms.
		\begin{tabular}{p{3cm}p{10cm}} 
        \multirow{2}{*}{\includegraphics[scale = 0.25]{./figures/aspirin.jpg}} & \\
		& $\vy=(\underbrace{+1}_{\text{heart}},\underbrace{+1}_{\text{stroke}},\underbrace{+1}_{\text{blood}},\underbrace{+1}_{\text{fever}},\underbrace{-1}_{\text{digest}},\underbrace{-1}_{\text{liver}},\underbrace{+1}_{\text{swelling}})$\\
        \end{tabular}
	\end{itemize}
\end{frame}

%
\begin{frame}{Flat multilabel classification approaches}
	\begin{itemize}\footnotesize
		\item The categorization is proposed in \cite{Tsoumakas10mining}
		\item Problem transformation
		\begin{itemize}\footnotesize
			\item Model the multilabel classification as a collection of single-label classification problems and solve each problem independently.
			\item For example, \mlknn\ \cite{Zhang07mlknn}, \cc\ \cite{Read09classifier,Read11classifier}, \iblr\ \cite{Cheng09combining}.
		\end{itemize}
		\item Algorithm adaptation 
		\begin{itemize}\footnotesize
			\item Modify the single-label classification algorithm for multilabel classification problems.
			\item For example, \adaboostmh\ \cite{Schapire99improved,Esuli2008boosting}, \corrlog\ \cite{Bian12corrlog}, \mtl\ \cite{Argyriou08convex}.
		\end{itemize}
		\item These approaches does not model the dependency structure explicitly.
	\end{itemize}
\end{frame}

%
\begin{frame}{Structured output prediction}
	\begin{itemize}\footnotesize
		\item Model the dependency structure with an output graph defined on microlabels.
		\item The categorization is proposed in \cite{Su2014Multilabel}.
		\item Hierarchical classification
		\begin{itemize}\footnotesize
			\item The output graph is a rooted tree or a \daggraph\ defining different levels of granularities.
			\item For example, \svmstruct\ \cite{THJA04,TJTA05}.
		\end{itemize}
		\item Graph labeling
		\begin{itemize}\footnotesize
			\item The output graph takes a more general form (e.g., a tree, a chain).
			\item For example, \crf\ \cite{lafferty01,taskar02}, \mmmn\ \cite{Taskar04max}, \mmcrf\ \cite{Rousu07, su10structured}, \spin\ \cite{su14structured}.
		\end{itemize}
		\item These approaches assume the output graph is known {\em apriori}.
	\end{itemize}
\end{frame}

%
\begin{frame}{Contributions}
	\begin{itemize}\footnotesize
		\item Structured output prediction models when the output graph is known.
		\begin{itemize}\footnotesize
			\item \spin\ for network influence prediction \cite{su14structured}.
			\item \mmcrf\ to work with general output graph structures \cite{su10structured}.
		\end{itemize}
		\item Structured output prediction models working with unknown output graph.
		\begin{itemize}\footnotesize
			\item \mve\ to combine multiple structured output predictors with ensemble \cite{su11mutitask}.
			\item \amm\ and \mam\ to aggregate the inference results from multiple structured output predictors \cite{su2013multilabelacml,su15multilabel}.
			\item \rta\ to perform joint learning and inference over a collection of random spanning trees \cite{su14multilabelnips}.
		\end{itemize}
		\item Codes for developed models are available from \href{http://hongyusu.github.io}{http://hongyusu.github.io}.
	\end{itemize}
\end{frame}


%
\begin{frame}{Outline}
	\begin{itemize}\footnotesize
		\item Preliminaries
		\item Structured output learning with known output graph
		\item Structured output learning with unknown output graph
		\item Future work
		\item Experimental results
	\end{itemize}
\end{frame}

%
\begin{frame}{Preliminaries}
	\begin{itemize}\footnotesize
		\item Training examples come in pairs $(\vx,\vy)\in\vXcal\times\vYcal$.
		\item $\vx\in\vXcal$ is an arbitrary input space.
		\item $\vYcal$ is an output space of a collection of $\ell$-dimensional {\em multilabels}.
		\begin{align*}\footnotesize
			\vy=(y_1,\cdots,y_{\ell})\in\vYcal.
		\end{align*}
		\item $y_i$ is a {\em microlabel} and $y_i\in\{1,\cdots,r_i\}, r_i\in\ZZ$.
		\item For example, multilabel binary classification $y_i\in\{-1,+1\}$.
		\item We are given a set of $m$ training examples $\{(\vx_i,\vy_i)\}_{i=1}^m$.
		%\item An arbitrary pair $(\vx_i,\vy),\,\vy_\in\vYcal$ is called pseudo-example.
		\item Each example $(\vx,\vy)$ is mapped into a joint feature space $\phib(\vx,\vy)$.
		\item $\vw$ is the weight vector in the joint feature space.
		\item Define a linear score function $F(\vw,\vx,\vy) = \ip{\vw}{\phib(\vx,\vy)}$.
		\item $\vw$ makes sure example $\vx$ with correct multilabel $\vy$ achieves higher score than with any other incorrect multilabel $\vy'\in\vYcal$.
	\end{itemize}
\end{frame}

\begin{frame}{Inference problem}
	\begin{itemize}
		\item The prediction $\vy_{\vw}(\vx)$ of an input $\vx$ is the multilabel $\vy$ that maximizes the score function 
		\begin{align}\footnotesize
			\vy_{\vw}(\vx) = \underset{\vy\in\vYcal}{\argmax}\,\ip{\vw}{\phib(\vx,\vy)}. \label{inference}
		\end{align}
		\item Search space $|\vYcal|=2^{\ell}$ is exponential in size.
		\item (\ref{inference}) is called {\em inference} problem which is \nphard\ for most output feature maps.
		\item We aim at using an output feature map in which the inference can be solved with a polynomial algorithm, e.g., dynamic programming.
	\end{itemize}
\end{frame}

%
\begin{frame}{Input-output feature maps}
	\begin{itemize}\footnotesize
		\item We assume that the joint feature map $\phib$ is a potential function on a Markov network $G=(E,V)$.
		\item A vertex $v_i\in V$ corresponds to a microlabel $y_i$, an edge $(v_i,v_j)\in E$ corresponds to the pairwise correlation of the microlabel $y_i$ and $y_j$.
		\item $G$ models potential pairwise correlations.
		\begin{center}
			\includegraphics[scale=0.3]{./figures/outputgraph.pdf}
		\end{center}
		\item $\varphib(\vx)\in\RR^d$ is the input feature map, e.g., bag-of-words of a document.
		\item $\psib(\vy)\in\RR^{4|E|}$ is the output feature map which maps the multilabel $\vy$ into a collection of edges and labels
		\begin{align*}\footnotesize
			\varphib(\vy) = (u_{e})_{e\in E},u_e\in\{-1,+1\}^2.
		\end{align*}
	\end{itemize}
\end{frame}

%
\begin{frame}{An example of output feature map}
	\begin{itemize}\footnotesize
		\item Markov network $G=(E,V)$
		\begin{center}
			\includegraphics[scale=0.3]{./figures/outputgraph.pdf}
		\end{center}
		\item Multilabel $\vy$
		\begin{align*}
			\vy&=(y_1,y_2,y_3,y_4)=(+1,-1,+1,+1)
		\end{align*}
		\item Output feature map $\psib(\vy)$
		\begin{align*}
			\psib(\vy) &= ( \underbrace{\underbrace{0}_{--}, \underbrace{0}_{-+}, \underbrace{1}_{+-}, \underbrace{0}_{++},}_{(v_1,v_3)} 
			\underbrace{\underbrace{0}_{--}, \underbrace{0}_{-+}, \underbrace{0}_{+-}, \underbrace{1}_{++},}_{(v_1,v_2)}
			\underbrace{\underbrace{0}_{--}, \underbrace{0}_{-+}, \underbrace{0}_{+-}, \underbrace{1}_{++}}_{(v_3,v_4)})
		\end{align*}
	\end{itemize}
\end{frame}

%
\begin{frame}{Joint feature map}
	\begin{itemize}\footnotesize
		\item The joint feature is the Kronecker product of $\varphib(\vx)$ and $\psib(\vy)$
		\begin{align*}\footnotesize
			\phib(\vx,\vy) = (\phib_e(\vx,\vy))_{e\in E}=(\varphib(\vx)\otimes\psib_e(\vy_e))_{e\in E}.
		\end{align*}
		\begin{center}
			\includegraphics[scale = 1]{./figures/tensor_label.pdf}
		\end{center}
		\item The score function can be factorized by the output graph $G$
		\begin{align*}
			F(\vw,\vx,\vy) = \ip{\vw}{\phib(\vx,\vy)} = \sum_{e\in E}\ip{\vw_e}{\phib_e(\vx,\vy_e)}.
		\end{align*}
	\end{itemize}
\end{frame}

%
\begin{frame}{Optimization problem}
	\begin{itemize}\footnotesize
		\item To learn parameter $\vw$, we aim to maximize the magin between correct pair $(\vx_i,\vy_i)$ and all the other incorrect pairs $(\vx_i,\vy),\vy\in\vYcal/\vy_i$ in the joint feature space $\phib$.
		\begin{center}
			\includegraphics[scale = 0.6]{./figures/jointfeaturespace.pdf}
		\end{center}
		\item The model is max-margin conditional random field \mmcrf\ \cite{Rousu07, su10structured}.
		\item The primal optimization problem is defined as
		\begin{align}
			\underset{\vw,\xi_k}{\minimize} & \quad \frac{1}{2}\norm{\vw}^2 + C\sum_{k=1}^{m}\xi_k \label{primalmmcrf}\\
			\st & \quad \ip{\vw}{\phib(\vx_k,\vy_k)} - \ip{\vw}{\phib(\vx_k,\vy)}  \geq \ell(\vy_k,\vy) -  \xi_k, \nonumber\\
			& \quad \xi_k\ge0\, , \forall\ \vy\in\vYcal, k \in \set{1,\dots,m}.\nonumber
		\end{align}
		\item $\ell(\vy,\vy_i)$ scales the margin according to the multilabel $\vy$.
		%\item (\ref{primalmmcrf}) is difficult as the number of the constraints is $m\times|\vYcal|$.
	\end{itemize}
\end{frame}

%
\begin{frame}{Marginal-dual optimization}
	\begin{itemize}\footnotesize
		\item (\ref{primalmmcrf}) is difficult as the number of the constraints is $m\times|\vYcal|$.
		\item The dual optimization problem is defined as
		\begin{align}
			\underset{\valpha\ge0}{\maximize} & \quad \valpha^{\tp}\vell - \frac{1}{2}\valpha^{\tp}K\valpha\label{mmcrfdual}\\
			\st & \quad \sum_{\vy\in\vYcal}\alpha(i,\vy)\le C, \, \forall i\in\{1,\cdots,m\}\nonumber.
		\end{align}
		\item (\ref{mmcrfdual}) is also challenging due to the exponential number of dual variables.
		\item We use edge marginals to replace the dual variables \cite{Taskar04max}
		\begin{align*}
			\mu(i,e,u_e) = \sum_{\vy}\ind{\psib_e(\vy)=u_e}\alpha(i,\vy). 
		\end{align*}
		\item The margin-dual optimization problem is 
		\begin{align}
			\underset{\vmu\in\Mcal}{\maximize} & \quad \vmu^{\tp}\ell - \frac{1}{2}\vmu^{\tp}K\vmu. \label{mmcrfmarginaldual}
		\end{align}
		\item The number of marginal-dual variable is $m\times4|E|$.
	\end{itemize}
\end{frame}

%
\begin{frame}{Conditional gradient optimization}
	\begin{itemize}\footnotesize
		\item (\ref{mmcrfmarginaldual}) is optimized by conditional gradient decent which optimizes $\mu_k$ that corresponds to a single example while keeps others ($\mu_j,j\neq k$) fixed 
		\begin{align*}
			\underset{\vmu_k\in\Mcal}{\maximize} & \quad \vmu_k^{\tp}\ell_k - \frac{1}{2}\sum_{j}\vmu_k^{\tp}K\vmu_j, \, \forall k.
		\end{align*}
		\item Current gradient of $\mu_k$ is given by $g_i = \ell_{i}-\sum_{j}K\mu_j$.
		\item Compute a feasible solution $\mu_k^*$ as an update direction
		\begin{align}
			\mu_k^* = \underset{\mu_k\in\Mcal}{\argmax} \, \mu_k^{\tp}g_k = \underset{\mu_k\in\Mcal}{\argmax} \, \sum_e\mu(k,e)^{\tp}g(k,e). \label{inferencemarginaldual}
		\end{align}
		\item (\ref{inferencemarginaldual}) is an instantiation of \map\ problem 
		\begin{itemize}\footnotesize
			\item $G$ is tree, exact inference with polynomial time algorithm, e.g, dynamic programming in \cite{Rousu07}
			\item $G$ is general graph, approximate inference, e.g. loopy belief propagation in \cite{su10structured}
		\end{itemize}
		\item Perform the update via exact line search $\mu_k \leftarrow \mu_k + \tau(\mu_k^*-\mu_k)$.
	\end{itemize}
\end{frame}

%
\begin{frame}{Exact line search}
	\begin{itemize}
		\item Line search gives the optimal feasible solution as a stationary point ($\tau$)
		\begin{align}
			\underset{\tau}{\maximize} &\quad g(\vmu_k + \tau\Delta\vmu_{k})\label{tree_line_search}\\
			\st &\quad 0\le\tau\le1.\nonumber
		\end{align}
		\item $\tau=0$ corresponds to no update.
		\item Feasible maximum update is achieved at $\tau=1$. 
		\item The cost of computing (\ref{tree_line_search}) is significantly smaller than the cost of computing (\ref{inferencemarginaldual}).
	\end{itemize}
\end{frame}

%
\begin{frame}{Compute duality gap}
	\begin{itemize}\footnotesize
		\item We use duality gap to measure the progress of the optimization.
		\item Primal and marginal-dual objective functions
		\begin{align*}\footnotesize
			f(\vw) &= \frac{1}{2}||\vw||^2 + C\sum_{k=1}^m\left(\ell_k-\ip{\vw}{\Delta\phib(\vx_k,\vy_k)}\right)\\
			g(\vmu) &= \sum_{k=1}^m\mu_k\ell_k - \frac{1}{2}\sum_{k=1}^m\sum_{j=1}^m\mu_kK^{\Delta\phib}(\vx_k,\vy_k;\vx_j,\vy_j)\mu_j
		\end{align*}
		\item $\underset{\vmu}{\maximize}\,g(\vmu)\le \underset{\vw}{\minimize}\,f(\vw)$, gap is minimized at optimal.
		\item Duality gap at $\vmu^t$
		\begin{align*}\footnotesize
			f(\vw^t) - g(\vmu^t) &= C\left(\vell-K^{\Delta\phib}\vmu^t\right) - \vmu^t\left(\vell-K^{\Delta\phib}\vmu^t\right)\\
			&= C\tp \nabla g(\vmu^t) -{\vmu^t} \tp \nabla g(\vmu^t)
		\end{align*}
	\end{itemize}
		\begin{enumerate}\footnotesize
			\item Estimate the marginal-dual objective by linear approximation $\nabla g(\vmu^t)$.
			\item Marginal-dual objective value at $\vmu^t$ is computed by ${\vmu^t} \tp \nabla g(\vmu^t)$.
			\item Primal objective value is estimate by $C\tp \nabla g(\vmu^t)$.
		\end{enumerate}
\end{frame}


%
\begin{frame}{So far in slides}
	\begin{itemize}\footnotesize
		\item We have been working with multilabel classification problems in general.
		\item We assume label correlation is described an output graph given {\em apriori}.
		\item We develop structured output prediction model utilizing the output graph. 
		\begin{itemize}\footnotesize
			\item Tree, the inference problem can be solved exactly with a polynomial time algorithm, e.g., dynamic programming.
			\item General graph, the inference problem is \nphard\ and can be solved with approximation algorithm, e.g., loopy belief propagation.
		\end{itemize}
		\item What if the output graph is not observed?
	\end{itemize}
\end{frame}
%
\begin{frame}{Research question}
	\begin{itemize}\footnotesize
		\item The output graph is hidden in many applications.
		\begin{itemize}\footnotesize
			\item For example, a surveillance photo can be tagged with ``building'', ``road'', ``pedestrian'', and ``vehicle''.
		\end{itemize}
		\item We study the problem in structured output learning when the output graph is not observed.
		\item In particular:
		\begin{itemize}\footnotesize
			\item Assume the dependency can be expressed by a complete set of pairwise correlations.
			\item Build a structured output learning model with a complete graph as the output graph.
			\item Solve the \nphard inference problem on the complete graph by a polynomial time algorithm.
		\end{itemize}
		\item A structured prediction model which performs max-margin learning on a random collection of spanning trees sampled from the output graph.
	\end{itemize}
\end{frame}

%
\begin{frame}{Complete graph as output graph}
	\begin{itemize}\footnotesize
		\item We assume that the joint feature map $\phib$ is a potential function on a Markov network $G=(E,V)$.
		\item $G$ is a complete graph with $|V| = \ell$ nodes and $|E| = \frac{\ell(\ell-1)}{2}$ undirected edges.
		\item $G$ models all pairwise correlations.
		\item $\varphib(\vx)$ is the input feature map, e.g., bag-of-words feature of an example $\vx$.
		\item $\psib(\vy)$ is the output feature map which is a collection of edges and labels
		\begin{align*}\footnotesize
			\varphib(\vy) = (u_{e})_{e\in E},u_e\in\{-1,+1\}^2.
		\end{align*}
		\item The joint feature is the Kronecker product of $\varphib(\vx)$ and $\psib(\vy)$
		\begin{align*}\footnotesize
			\phib(\vx,\vy) = (\phib_e(\vx,\vy))_{e\in E}=(\varphib(\vx)\otimes\psib_e(\vy_e))_{e\in E}.
		\end{align*}
		\item The score function can be factorized by the complete graph $G$
		\begin{align*}
			F(\vw,\vx,\vy) = \ip{\vw}{\phib(\vx,\vy)} = \sum_{e\in E}\ip{\vw_e}{\phib_e(\vx,\vy_e)}.
		\end{align*}
	\end{itemize}
\end{frame}

\begin{frame}{Inference in terms of all spanning trees}
	\begin{itemize}
		\item Solving the following inference problem on a complete graph is \nphard
		\begin{align*}
			\vy_{\vw}(\vx) = \underset{\vy\in\vYcal}{\argmax}\,F(\vw,\vx,\vy)  = \underset{\vy\in\vYcal}{\argmax}\,\sum_{e\in E}\ip{\vw_e}{\phib_e(\vx,\vy_e)}. 
		\end{align*}
		$\phi_G(\vx,\vy) = \{\phi_{G,e}(\vx,\vy_e)\}_{e\in G},\vw_G = \{\vw_{G,e}\}_{e\in G},||\phi_G(\vx,\vy)||=||\vw_{G}||=1$
		\item For a complete graph, there are $\ell^{\ell-2}$ unique spanning trees.
		\item $\phi_T(\vx,\vy)=\{\phi_e(\vx,\vy)\}_{e\in T}$ is the projection of $\phi_G(\vx,\vy)$ on $T\in S(G)$.
		\item $\vw_{T}=\{\vw_{G,e}\}_{e\in T}$ is the projection of $\vw_G$ on $T\in S(G)$.
		\item We can write $F(\vw,\vx,\vy)$ as a conic combination of all spanning trees
		\begin{align*}
			F(\vw,\vx,\vy) &= \underset{T\in U(G)}{\E}a_T\ip{\vw_T}{\phib_T(\vx,\vy)}\\
			  &\underset{T\in U(G)}{\E}a_T^2=1,  \underset{T\in U(G)}{\E}a_T<1.
		\end{align*}
		\item $U(G)$ is the uniform distribution over $\ell^{\ell-2}$ spanning trees.
		\item The number of spanning trees is exponentially dependent on the number of nodes $\ell$.
	\end{itemize}
\end{frame}

%
\begin{frame}{A sample of $n$ spanning trees}
	\begin{itemize}\footnotesize
		\item Instead of using all spanning trees, we can just use $n$ spanning trees
		\begin{align*}\footnotesize
			F_{\Tcal}(\vw,\vx,\vy) &= \frac{1}{n}\sum_{i=1}^{n}a_{T_i}\ip{\vw_{T_i}}{\phib_{T_i}(\vx,\vy)}\\
			  &\frac{1}{n}\sum_{i=1}^{n}a_{T_i}^2=1,  \frac{1}{n}\sum_{i=1}^{n}a_{T_i}<1.
		\end{align*}
		\item When
		\begin{align*}\footnotesize
			n\ge\frac{\ell^2}{\epsilon^2}(\frac{1}{16}+\frac{1}{2}\ln\frac{8\sqrt{n}}{\delta}),
		\end{align*}
		we have $|F_{\Tcal}(\vw,\vx,\vy)-F(\vw,\vx,\vy)|\le\epsilon$, with high probability.
		\item A sample of $n\in\Theta(\ell^2/\delta^2)$ random spanning tree is sufficient to estimate the score function.
		\item Margin achieved by $F(\vw,\vx,\vy)$ is also preserved by the sample of $n$ random spanning trees $F_{\Tcal}(\vw,\vx,\vy)$ \cite{su14multilabelnips}.
	\end{itemize}
\end{frame}

%
\begin{frame}{Random spanning tree approximation \rta}
	\begin{itemize}\footnotesize
		\item The optimization problem of \rta\ is defined as \cite{su14multilabelnips}
		\begin{align*}
			\underset{\vw_{T_i},\xi_i}{\minimize} & \quad \frac{1}{2}\sum_{i=1}^{n}\norm{\vw_{T_i}}^2 + C\sum_{k=1}^{m}\xi_k\\
			\st & \quad \frac{1}{\sqrt{n}}\sum_{i=1}^{n}{ \langle \vw_{T_i}, \phib_{T_t}(\vx_k,\vy_k) \rangle} - \underset{\vy \neq \vy_k}{\maximize\ } \frac{1}{\sqrt{n}}\sum_{i=1}^{n}{\langle \vw_{T_t}, \phib_{T_i}(\vx_k,\vy) \rangle } \geq 1 -  \xi_k, \\
			& \quad \xi_k\ge0\, , \forall\ k \in \set{1,\dots,m}.
		\end{align*}
		\item The marginal-dual form is given by
		\begin{align*}\footnotesize
			\underset{\vmu\in\Mcal}{\maximize} & \quad \sum_{i=1}^{n}\left( \vmu_{T_i}\vell_{T_i} - \frac{1}{2}\vmu_{T_i}K^{\Delta\phib}_{T_i}\vmu_{T_i}\right)\\
			\st & \quad \sum_{u_e}\vmu_{T_i,e}(u_e)\le C.
		\end{align*}
		\item Inside the summation, there is a structure output model with parameter $\vmu_{T_i}$ defined on a spanning tree $T_i$.
		\item The problem is how to jointly optimize structured output models defined on $n$ spanning trees.
	\end{itemize}
\end{frame}

%
\begin{frame}{Inference Problem for a collection of trees}
	\begin{itemize}
		\item The inference problem of \rta\ is defined as finding the multilabel $\vy_{\Tcal}(\vx)$ that maximizes the sum of scores over a collection of trees
		\begin{align*}
			\vy_{\Tcal}(\vx) = \underset{\vy\in\vYcal}{\argmax}\,{\color{aaltoblue}F_{\Tcal}(\vx,\vy;\vw_{\Tcal})} = \underset{\vy\in\vYcal}{\argmax}\,\sum_{t=1}^{n}\ip{\vw_{T_t}}{\phi_{T_t}(\vx,\vy)}.
		\end{align*}
		\item The inference problem on each individual spanning tree can be solve efficiently in $\Theta(\ell)$ by \textit{dynamic programming}
		\begin{align*}
			\vy_{T_t}(\vx) = \underset{\vy\in\vYcal}{\argmax}\,{\color{aaltored}F_{T_t}(\vx,\vy;\vw_{T_t})}= \underset{\vy\in\vYcal}{\argmax}\,\ip{\vw_{T_t}}{\phi_{T_t}(\vx,\vy)}.
		\end{align*}
		\item There is no guarantee that there exists a tree $T_t\in\Tcal$ in which the maximizer of ${\color{aaltored}F_{T_t}}$ is the maximizer of ${\color{aaltoblue}F_{\Tcal}}$.
	\end{itemize}
\end{frame}

%
\begin{frame}[allowframebreaks]{Fast inference for a collection of trees}
	\begin{itemize}
		\item For each tree $T_t$, instead of computing the best multilabel $\vy_{T_t}$, we compute $K$-best multilabels in $\Theta(K\ell)$ time
		\begin{align*}
			\Ycal_{T_t,K} = \{\vy_{T_t,1},\cdots,\vy_{T_t,K}\}.
		\end{align*}
		\item Performing the same computation on all trees gives a candidate list of $n\times K$ multilabels (K best list) in $\Theta(nK\ell)$ time
		\begin{align*}
			\Ycal_{\Tcal,K}=\Ycal_{T_1,K}\cup\cdots\Ycal_{T_n,K}.
		\end{align*}
		\item We prove that with high probability the global best multilabel will exist in K best list.
		\item We have developed a condition to verify the global best multilabel from K best list in linear time $\Theta(nK)$.
	\end{itemize}
\end{frame}


\begin{frame}{Exact line search for a single tree}
	\begin{figure}
		\begin{center}
			\includegraphics[scale=0.3]{optimization_single_tree.pdf}
		\end{center}
	\end{figure}
	\begin{itemize}
		\item Line search gives the optimal feasible solution as a stationary point ($\tau$)
		\begin{align}
			\underset{\tau}{\maximize} &\quad f(\vmu_{T_i}^k + \tau\Delta\vmu_{T_i}^{k})\label{tree_line_search}\\
			\st &\quad 0\le\tau\le1.\nonumber
		\end{align}
		\item $\tau=0$ corresponds to no update.
		\item Feasible maximum update is achieved at $\tau=1$. 
	\end{itemize}
\end{frame}

\begin{frame}{Optimization on a collection of $n$ spanning trees}
	\begin{figure}
		\begin{center}
			\includegraphics[scale=0.3]{best_update.pdf}
		\end{center}
	\end{figure}
\end{frame}

\begin{frame}{Exact line search for the collection of trees}
	\begin{itemize}
		\item The step size along the update direction $\tau$ is given by the exact line search
		\begin{align*}\footnotesize
			\underset{\tau}{\maximize} &\quad {\color{aaltored}\sum_{i=1}^n}f(\vmu_{T_i}^k + \tau\Delta\vmu_{T_i}^{k})\\
			\st &\quad 0\le\tau\le1.
		\end{align*}
		\item Problems with the {\em best update}
		\begin{enumerate}\footnotesize
			\item The best feasible solution on a single tree might not be the best feasible solution on a collection of trees
			\begin{align*}
				{\vmu}_{T}^{k,*}\notin{\color{aaltored}{}{\vmu}_{T_i}^{k,*}{\color{aaltored}}_{i=1}^n}.
			\end{align*}
			\item $\kappa$-best inference algorithm
			\begin{align*}
				({\vmu}_{T_i}^{k,*_h})_{h=1}^{\kappa} &= \underset{\vmu\in\Mcal}{\argmax}\,\vmu\tp g_{T_i}^k,\,{\color{aaltored}\forall i}\\
				{\vmu}_{T}^{k,*} &\in {\color{aaltored}{}{\vmu}_{T_i}^{k,*_h}{\color{aaltored}}_{i=\{1,\cdots,n\},h\in\{1,\cdots,\kappa\}}}.
			\end{align*}
		\end{enumerate}
	\end{itemize}
\end{frame}

\begin{frame}{Update with multiple directions}
	\begin{figure}
		\begin{center}
			\includegraphics[scale=0.3]{multiple_update.pdf}
		\end{center}
	\end{figure}
\end{frame}

\begin{frame}{Newton method to compute $\vtau$}
	\begin{itemize}\footnotesize
		\item We want to find $\tau$ that maximize the objective function given the update
		% \begin{align*}\footnotesize
		% 	&f(\vmu^{G,k} + \Delta{\vmu}^{G,k+1}) \\
		% 	&= (\vmu^{G,k} + \Delta{\vmu}^{G,k+1})\tp\ell_G - \frac{1}{2}(\vmu^{G,k} + \Delta{\vmu}^{G,k+1})\tp K_G (\vmu^{G,k} + \Delta{\vmu}^{G,k+1}).
		% \end{align*}
		\begin{align*}
			\underset{\tau}{\maximize} & \quad f(\vmu^{G,k} + \Delta{\vmu}^{G,k+1})\\
			\st & \quad 0\le\tau_i\le1,\,\sum_{i=1}^n\tau_i\le1,\,\forall i.
		\end{align*}
		\item The objective is quadratic with respect to $\vtau$.
		\item We use Newton method to find $\vtau$ that maximize the objective.
		\item $\vtau$ is projected into the feasible region.
	\end{itemize}
\end{frame}


%
\begin{frame}{Performance of the Inference Algorithm}
	\begin{itemize}\footnotesize
		\item $10$ datasets, $|\Tcal| = \{5,10,40\}, K=\{2,4,8,16,32,40,60\}$
		\item Y-axis is the percentage of examples with exact inference.
		\item X-axis is the value of $K$ as the percentage of the number of microlabels.
		\item $K=100\%|Y|$ corresponds to a complexity of $\Theta(nl^2)$.
	\end{itemize}
	\begin{figure}
		\begin{center}
			\includegraphics[width=11cm]{./result_plot.pdf}
			%\caption{Percentage of examples with provably optimal $\vy$ being in the $K$-best lists plotted as a function of K, scaled with respect to the number of microlabels in the dataset.}
		\end{center}
	\end{figure}
\end{frame}



%
\begin{frame}{\rta\ on multilabel benchmark datasets}
	\begin{figure}\footnotesize
		\begin{center}
			\includegraphics[width=11cm]{./result_table.pdf}
			\caption{\footnotesize Prediction performance of each algorithm in terms of microlabel loss and 0/1 loss. The best performing algorithm is highlighted with boldface, the second best is in italic}
		\end{center}
	\end{figure}
\end{frame}




%
\begin{frame}{Conclusion}
	\begin{itemize}\footnotesize
		\item Structured output prediction in multilabel classification problems.
		\item Utilize label correlation described by an output graph to make accuracy predictions.
		\item We focus on the problems where the output graph is unknown.
		\item We model the complete pairwise correlation with an complete graph.
		\item We approach the \nphard\ inference problem on the complete graph by a collection of its spanning trees.
		\item The proposed model has better performance on multilabel benchmark datasets.
		\item 
	\end{itemize}
\end{frame}






\begin{frame}[allowframebreaks]{Bibliography}
%\bibliographystyle{plain}
\bibliographystyle{apalike}
 \bibliography{dissertation}
\end{frame}

\end{document}
